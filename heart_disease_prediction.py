# -*- coding: utf-8 -*-
"""Heart_disease_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zi8Ia2oe9GahXf1Ft7qqaHjoeh09VKI

**Heart disease prediction**

1.**Dataset**

   This is a multivariate type of dataset which means providing or involving a variety of separate mathematical or statistical variables, multivariate numerical data analysis. It is composed of 14 attributes which are age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise-induced angina, oldpeak â€” ST depression induced by exercise relative to rest, the slope of the peak exercise ST segment, number of major vessels and Thalassemia.

Column Descriptions:

id (Unique id for each patient)

age (Age of the patient in years)

dataset (place of study)

sex (Male/Female)

cp Chest pain type:
typical angina,
atypical angina,
non-anginal,
asymptomatic,

trestbps resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))

chol (serum cholesterol in mg/dl)

fbs (if fasting blood sugar > 120 mg/dl)

restecg (resting electrocardiographic results)

Values: [normal, stt abnormality, lv hypertrophy]

thalach: maximum heart rate achieved

exang: exercise-induced angina (True/ False)

oldpeak: ST depression induced by exercise relative to rest

slope: the slope of the peak exercise ST segment

ca: number of major vessels (0-3) colored by fluoroscopy

thal: [normal; fixed defect; reversible defect]

num: the predicted attribute

**Aim**: To perform exploratory data analysis (EDA) and build a machine learning model that accurately predicts heart disease, helping in early detection and improving healthcare decisions.

**Objectives**:

1.Explore the Data: Analyze the dataset to understand key patterns and relationships through EDA.

2.Feature Engineering: Create or modify features to improve model accuracy.

3.Model Building: Develop and compare various machine learning models.

4.Model Evaluation: Measure model performance using accuracy, precision, recall, and F1-score.

5.Insights: Identify the key factors contributing to heart disease and provide actionable insights.

**Import libraries**
"""

#To handle the data
import pandas as pd
import numpy as np

# to visualize the dataset
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# To preprocess the data
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
# import iterative imputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# machine learning
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
#for classification tasks
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestRegressor
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB
#pipeline
from sklearn.pipeline import Pipeline
#metrics
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error, mean_squared_error, r2_score

# ignore warnings
import warnings
warnings.filterwarnings('ignore')

"""**Load the dataset**"""

#load the dataset placed in our local pc
df = pd.read_csv('/content/drive/MyDrive/heart_disease_uci.csv')

#display the first 5 rows of the dataset
df.head()

"""**Exploratory Data Analysis (EDA)**"""

#exploring data types of the dataset
df.info()

#data shape
df.shape

"""ID Column"""

#id column
df['id'].min(), df['id'].max()

"""id column is a unique identifier for each patient, it is not useful for our analysis.

Age Column
"""

#age column
df['age'].min(), df['age'].max()

"""Observation: The minimum age of the patient is 28 years"""

# summarize age column
df['age'].describe()

#histogram to see the distribution of age
sns.histplot(df['age'], kde=True)
plt.title('Age Distribution')

#mean, median, & mode of age column
sns.histplot(df['age'], kde=True)
plt.axvline(df['age'].mean(), color='red')
plt.axvline(df['age'].median(), color='green')
plt.axvline(df['age'].mode()[0], color='blue')

# print the value of mean, median and mode of age column
print('Mean:', df['age'].mean())
print('Median:', df['age'].median())
print('Mode:', df['age'].mode()[0])

"""Explore Gender Distribution based on age"""

#histogram to see the distribution of gender on age using plotly
fig = px.histogram(df, x='age', color='sex')
fig.show()

"""Sex Column"""

#value counts of gender
df['sex'].value_counts()

#Male & female percenatge in our dataset
male_count = 726
female_count = 194
total_count = male_count + female_count

# calculate percentages
male_percentage = (male_count / total_count) * 100
female_percentage = (female_count / total_count) * 100

# display the results
print(f"Male percentage in the data: {male_percentage:.2f}%")
print(f"Female Percentage in the data: {female_percentage:.2f}%")

# difference
difference_percentage = ((male_count - female_count) / female_count) * 100
print(f"Males are {difference_percentage:.2f}% more than females in the data.")

"""Dataset Column"""

#dataset column
df['dataset'].unique()

#value counts in dataset column
df['dataset'].value_counts()

# Create a bar chart with counts
fig = px.bar(df, x='dataset', color='sex', barmode='group')

# Add counts as text labels
df_counts = df.groupby(['dataset', 'sex']).size().reset_index(name='count')
fig = px.bar(df_counts, x='dataset', y='count', color='sex', barmode='group', text='count')


fig.show()

#plot the distribution of age on dataset
fig = px.histogram(df, x='age', color='dataset')
fig.show()

#mean, median, & mode of age column on dataset column
print(f"Mean of age based on dataset: {df.groupby('dataset')['age'].mean()}")
print("-------------------------------------")
print(f"Median of age based on dataset: {df.groupby('dataset')['age'].median()}")
print("-------------------------------------")
print(f"Mode of age based on dataset: {df.groupby('dataset')['age'].agg(pd.Series.mode)}")
print("-------------------------------------")

"""CP (Chest Pain) Column"""

#value counts of chest pain column
df['cp'].value_counts()

#plot the cp column using sns
sns.countplot(data=df, x='cp', hue='sex')
plt.title('Chest Pain Distribution based on Sex')
plt.show()

#plot the cp based on dataset column
sns.countplot(data=df, x='cp', hue='dataset')
plt.title('Chest Pain Distribution based on Dataset')
plt.show()

#plot the cp based on age column using plotly
fig = px.histogram(df, x='age', color='cp')
fig.show()

"""Dealing with missing values"""

df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)

missing_data_cols = df.isnull().sum()[df.isnull().sum() > 0].index.tolist()
missing_data_cols

categorical_cols = ['thal', 'ca', 'slope', 'exang', 'restecg','fbs', 'cp', 'sex', 'num']
bool_cols = ['fbs', 'exang']
numeric_cols = ['oldpeak', 'thalch', 'chol', 'trestbps', 'age']

# define the function to impute the missing values

def impute_categorical_missing_data(passed_col):

    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]

    other_missing_cols = [col for col in missing_data_cols if col != passed_col]

    label_encoder = LabelEncoder()

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    if passed_col in bool_cols:
        y = label_encoder.fit_transform(y)

    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    rf_classifier = RandomForestClassifier()

    rf_classifier.fit(X_train, y_train)

    y_pred = rf_classifier.predict(X_test)

    acc_score = accuracy_score(y_test, y_pred)

    print("The feature '"+ passed_col+ "' has been imputed with", round((acc_score * 100), 2), "accuracy\n")

    X = df_null.drop(passed_col, axis=1)

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    if len(df_null) > 0:
        df_null[passed_col] = rf_classifier.predict(X)
        if passed_col in bool_cols:
            df_null[passed_col] = df_null[passed_col].map({0: False, 1: True})
        else:
            pass
    else:
        pass

    df_combined = pd.concat([df_not_null, df_null])

    return df_combined[passed_col]

def impute_continuous_missing_data(passed_col):

    df_null = df[df[passed_col].isnull()]
    df_not_null = df[df[passed_col].notnull()]

    X = df_not_null.drop(passed_col, axis=1)
    y = df_not_null[passed_col]

    other_missing_cols = [col for col in missing_data_cols if col != passed_col]

    label_encoder = LabelEncoder()

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(random_state=42), add_indicator=True)

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    rf_regressor = RandomForestRegressor()

    rf_regressor.fit(X_train, y_train)

    y_pred = rf_regressor.predict(X_test)

    print("MAE =", mean_absolute_error(y_test, y_pred), "\n")
    print("RMSE =", mean_squared_error(y_test, y_pred), "\n")
    print("R2 =", r2_score(y_test, y_pred), "\n")

    X = df_null.drop(passed_col, axis=1)

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype == 'category':
            X[col] = label_encoder.fit_transform(X[col])

    for col in other_missing_cols:
        if X[col].isnull().sum() > 0:
            col_with_missing_values = X[col].values.reshape(-1, 1)
            imputed_values = iterative_imputer.fit_transform(col_with_missing_values)
            X[col] = imputed_values[:, 0]
        else:
            pass

    if len(df_null) > 0:
        df_null[passed_col] = rf_regressor.predict(X)
    else:
        pass

    df_combined = pd.concat([df_not_null, df_null])

    return df_combined[passed_col]

df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)

#using our function to impute the missing values using for loop
for col in missing_data_cols:
    print("Missing Values", col, ":", str(round((df[col].isnull().sum() / len(df)) * 100, 2))+"%")
    if col in categorical_cols:
        df[col] = impute_categorical_missing_data(col)
    elif col in numeric_cols:
        df[col] = impute_continuous_missing_data(col)
    else:
        pass

#check if there are any missing values
df.isnull().sum()

"""Missing values are imputed.

Dealing with Outliers
"""

#box plot of all numeric columns using for loop
plt.figure(figsize=(20, 20))

colors = ['red', 'green', 'blue', 'orange', 'purple']

for i, col in enumerate(numeric_cols):
    plt.subplot(3, 2, i+1)
    sns.boxplot(x=df[col], color=colors[i])
    plt.title(col)
plt.show()

#plot box plot for all numeric columns using for loop in plotly
fig = px.box(df, y='age', title='Age Box Plot')
fig.show()

fig = px.box(df, y='trestbps', title='Trestbps Box Plot')
fig.show()

fig = px.box(df, y='chol', title='Chol Box Plot')
fig.show()

fig = px.box(df, y='thalch', title='Thalach Box Plot')
fig.show()

fig = px.box(df, y='oldpeak', title='Oldpeak Box Plot')
fig.show()

# defining a function for outlier treatment using z-score
def outlier_treatment(df , col):

    # Calculate the Z-scores for each value in the column
    z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())

    # Define the threshold for identifying outliers
    threshold = 3

    # identify rows where any column has a Z-score above the threshold
    outliers = (z_scores > threshold)

    # the number of rows identified as outliers
    print(f'Number of rows identified as outliers in {col}: {outliers.sum()}')

    # Remove the outliers
    df = df[~outliers]

    # print statement
    print('Z score has been successfully applied on {}.'.format(col))

    # returning the dataframe
    return df

# aaplying outlier_treatment function on trestbps
df = outlier_treatment(df , 'trestbps')

# applying outlier_treatment function on chol
df = outlier_treatment(df , 'chol')

# # Dropping rows where 'trestbps' or 'chol' are 0, as these values are not medically possible.
df = df[df['chol'] != 0]

# check the row where trestbps is 0
df[df['trestbps']==0]

# Remove the row where trestbps is not equal to zero
df=df[df['trestbps']!=0]
df.head()

df.info()

# setting up the figure size
plt.figure(figsize=(15, 4))
colors = ['red', 'green', 'blue', 'orange', 'purple']

# loop through each column
for i in range(len(numeric_cols)):
    # create a subplot
    plt.subplot(1, len(numeric_cols), i + 1)
    # plotting the boxplot
    sns.boxplot(y=df[numeric_cols[i]], color=colors[i])
    # adding title
    plt.title(f'Boxplot of "{numeric_cols[i]}" \n after outlier treatment')

plt.tight_layout()

plt.show()

df.info()

"""Resting Blood Pressure (trestbps) Column

The normal resting blood pressure is 120/80 mm Hg.

1.High BP (Hypertension): Can lead to heart disease, stroke.

2.Low BP (Hypotension): May cause dizziness, fainting.
"""

#summary statistics of trestbps column
df['trestbps'].describe()

#histogram of trestbps column
sns.histplot(df['trestbps'], kde=True)
plt.title('Resting Blood Pressure Distribution')
plt.show()

df['trestbps'].value_counts().nlargest(5)

"""Observation: Majority of the Patients have Resting Blood pressure ranges from 110-150 mm Hg."""

#Plot the distribution of trestbps based on gender
fig = px.histogram(df, x='trestbps', color='sex')
fig.show()

"""According to our dataset, Females have higher resting blood pressure as compared to males.

Chol Column

The normal cholesterol level is less than 200 mg/dL
"""

df['chol'].describe()

#plot the chol column
sns.histplot(df['chol'], kde=True)
plt.title('Cholestrol Distribution')
plt.show()

df['chol'].value_counts().nlargest(5)

"""observation: The majority of the patients have cholesterol levels between 200-300 mg/dl. Which is slightly higher than the normal range."""

#Age Column binning
df['age_bins'] = pd.cut(df['age'], bins=[0, 30, 40, 50, 60, 70, 80], labels=['0-30', '31-40', '41-50', '51-60', '61-70', '71-80'])

df['age_bins'].value_counts()

df.columns

sns.barplot(data=df, x='age_bins',y='chol', hue='sex', palette='rocket')

#which category has the highest cholestrol
df.groupby('age_bins')['chol'].median().sort_values(ascending=False)

"""The cholesterol level is highest among the age group of 61-70 years.

FBS Column

bs column tells us about the fasting blood sugar levels of the patients.
"""

df['fbs'].value_counts()

#make a good plot of fbs column using sns
sns.countplot(data=df, x='fbs', hue='dataset', palette='viridis')
plt.title('Fasting Blood Sugar Distribution based on Dataset')
plt.show()

"""Observation: The majority of the patients have fasting blood sugar levels less than 120 mg/dl.

Restecg Column
"""

df['restecg'].value_counts()

"""Normal:A healthy ECG reading with no signs of heart problems.

LV Hypertrophy: Thickening of the heart's left side, which can happen when the heart works too hard.

ST-T Abnormality: Unusual patterns in part of the ECG that may point to heart issues like reduced blood flow or heart attack.
"""

#plot restecg using plotly count plot
fig = px.histogram(df, x='age_bins', color='restecg', barmode='group', title='Resting ECG Results Based on Age')
fig.show()

"""Observation: According to our dataset, majority of the patients have normal Resting ECG but some patients have ST-T wave abnormality. which may indicate heart issues

Thalch Column
"""

df['thalch'].value_counts().nlargest(5)

#groupby thalch based on age_bins
average_thalch = df.groupby('age_bins')['thalch'].mean().sort_values(ascending=False)
print(average_thalch)
#plotting it
plt.figure(figsize=(8, 6))
sns.barplot(x=average_thalch.index, y=average_thalch.values, palette='viridis')
plt.title('Average Maximum Heart Rate by Age Bins')
plt.xlabel('Age Bins')
plt.ylabel('Average Max Heart Rate (bpm)')
plt.show()

"""Observation: The young age group has a higher heart rate as compared to the older age group

Exang Column

This Column indicates whether a person experiences angina (chest pain) during physical exertion.

True: The individual experiences angina when exercising.
False: The individual does not experience angina when exercising
"""

df['exang'].value_counts()

sns.countplot(data=df,x='exang',hue='age_bins')

"""Observation: According to our dataset, the majority of the patients does not experience angina during physical exertion but age group 51-60 has the highest number of patients who experience angina during physical exertion.

Oldpeak Column

1. It indicates how much the ST segment falls below the baseline during exercise.\ 2. A higher oldpeak value suggests more significant ST depression, which can indicate myocardial ischemia (reduced blood flow to the heart).
"""

df['oldpeak'].value_counts().nlargest(5)

"""0: No ST depression (normal, healthy heart response).

0 to 1 mm: Mild ST depression, usually not concerning but can be observed in some cases.

Greater than 1 mm: Clinically significant ST depression, which may indicate myocardial ischemia (reduced blood flow to the heart) and is often associated with coronary artery disease.
"""

#groupby oldpeak based on age_bins
df.groupby('age_bins')['oldpeak'].mean().sort_values(ascending=False)

#plot oldpeak column based on age_bins using sns
sns.barplot(data=df, x='age_bins', y='oldpeak', palette='viridis',hue='sex')
plt.title('Oldpeak Distribution based on Age Bins')
plt.show()

"""Observations:

ST depression (oldpeak) rises with age, showing a higher risk of heart issues in older age groups.

Age groups 51-80 have average oldpeak values over 1 mm, indicating clinically significant heart stress.

The 61-70 group has the highest average oldpeak (1.52 mm), suggesting increased heart disease risk in this age bracket.
Males have higher oldpeak values as compared to Femlaes.

Slope column

Flat (427 cases): Most common slope, indicating a higher likelihood of ischemia.
Upsloping (254 cases): Suggests healthier heart function; less concerning.
Downsloping (59 cases): Least common but most alarming, indicating severe heart disease.
"""

df['slope'].value_counts()

#groupby slope based on age_bins
df.groupby('age_bins')['slope'].value_counts()

#plot the slope column based on age_bins using sns
sns.countplot(data=df, x='slope', hue='age_bins')
plt.title('Slope Distribution based on Age Bins')
plt.show()

"""Observations:

Younger Age Groups (0-40): Primarily exhibit upsloping and flat slopes, suggesting relatively healthier heart responses.

Middle Age Groups (41-60): Higher counts of flat slopes (up to 177) indicate an increase in potential ischemia risk.

Older Age Groups (61-80): A mix of slopes, with a notable presence of downsloping (8 cases in 71-80), indicating a concerning trend toward severe heart conditions.

CA Column

0: No major vessels colored (indicating no visible blockages).

1: One major vessel colored.

2: Two major vessels colored.

3: Three major vessels colored (indicating significant blockage or severe coronary artery disease).
"""

df['ca'].value_counts()

#groupby ca based on age_bins & sex
df.groupby(['age_bins', 'sex'])['ca'].count().reset_index()

#plot ca based on age_bins
sns.barplot(data=df, x='age_bins',y='ca',hue='sex', palette='Set2')

"""Observations:

Males show more affected vessels across all age bins.

Significant rise in affected vessels with age, especially in males aged 51-60 (233 cases).

Few cases in the 0-30 age group (1 female, 4 male).

Notable increase in the 41-50 age group (144 males).

Thal Column

Helps diagnose coronary artery disease and guides treatment decisions based on blood flow patterns.
"""

df['thal'].value_counts()

"""Reversible Defect (353 cases): Indicates temporary reduced blood flow during stress, suggesting ischemia.

Normal (325 cases): Shows normal blood flow, indicating no significant heart disease.

Fixed Defect (62 cases): Indicates permanent reduced blood flow, suggesting previous heart damage.
"""

df.groupby(['age_bins', 'sex'])['thal'].size().reset_index()

sns.countplot(data=df, x='age_bins', hue='thal', palette='Set2')

sns.countplot(data=df, x='sex', hue='thal', palette='Set2')

"""Observations:

Very few cases (1 female, 4 males) in 0-30 age group, indicating low heart disease risk.


Significant increase in 41-50 age group, especially in males (144 cases), indicating higher risk.

Highest count in 51-60 age group (233 males), suggesting urgent monitoring.

Females consistently show lower counts across all age bins.

Notable cases in 61-70 age group (32 females, 106 males), highlighting increased risk.

Num Column
"""

df['num'].value_counts()

"""0 = no heart disease

1 = mild heart disease

2 = moderate heart

3 = severe heart disease

4 = critical heart disease
"""

df.groupby(['age_bins', 'sex', 'num']).size().reset_index(name='count')

#plot num column on age_bins
sns.countplot(data=df, x='num', hue='sex', palette='viridis')

"""Observations:

Minimal heart disease predictions in the 0-30 age group.

Significant cases (especially num values 1 and 2) in the 41-50 age group.

Severe predictions rise in 51-60 and 61-70 age groups for males.

Females show fewer predicted heart disease cases overall.

**Machine Learning**

The Target Column is num which is the predicted attribute. We will use this column to predict the heart disease. The unique values in this column are: [0, 1, 2, 3, 4], which states that there are 5 types of heart diseases.

0 = no heart disease

1 = mild heart disease

2 = moderate heart disease

3 = severe heart disease

4 = critical heart disease

For this project, we will convert the num column into a binary classification problem. We will consider the following values:

0 = no heart disease

1 = heart disease

It will make easy a model to predict the heart disease.
"""

#split the data into X and Y
X = df.drop(['num','id'], axis=1)
y = df['num']
#target engineering on num
y = np.where((y == 1) | (y == 2) | (y == 3) | (y == 4), 1,0)

label_encoder = LabelEncoder()

for col in X.columns:
    if X[col].dtype == 'object' or X[col].dtype == 'category':
        X[col] = label_encoder.fit_transform(X[col])
    else:
        pass

print(numeric_cols)

#Scaling numeric columns
min_max_scaler = MinMaxScaler()
X[numeric_cols] = min_max_scaler.fit_transform(X[numeric_cols])

#plot all numeric columns
plt.figure(figsize=(20,20))
for i, col in enumerate(numeric_cols):
    plt.subplot(3,2, i+1)
    sns.histplot(X[col], kde=True)
    plt.title(col)

# split the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""The following models will be used to predict the heart disease:

1.Random Forest

2.Support Vector Machine (SVM)

3.Logistic Regression

4.K-Nearest Neighbors (KNN)

5.Decision Tree

"""

# Define models and hyperparameters
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Support Vector Machine': SVC(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42)
}

params = {
    'Random Forest': {
        'model__n_estimators': [100, 200, 300],
        'model__max_depth': [ 10,20],
        'model__min_samples_split': [2, 5]
    },
    'Support Vector Machine': {
        'model__C': [1, 10],
        'model__gamma': [0.1, 0.01]
    },
    'Logistic Regression': {
        'model__C': [1, 10],
        'model__solver': ['lbfgs', 'liblinear']
    },
    'K-Nearest Neighbors': {
        'model__n_neighbors': [3, 5],
        'model__weights': ['uniform', 'distance']
    },
    'Decision Tree': {
        'model__criterion': ['gini', 'entropy'],
        'model__max_depth': [None, 10],
        'model__min_samples_split': [2, 5]
    }
}

# Initialize best model tracking
best_model = None
best_accuracy = 0.0

# Train and evaluate each model
for name, model in models.items():
    print(f"Training {name}...")

    # Create a pipeline with the model
    pipeline = Pipeline([
        ('model', model)
    ])

    # Get hyperparameters for the current model
    model_params = params.get(name, {})

    # Create GridSearchCV with the pipeline and parameters
    grid_search = GridSearchCV(pipeline, model_params, cv=5, n_jobs=-1, verbose=0)

    # Fit the pipeline
    grid_search.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = grid_search.predict(X_test)

    # Print evaluation metrics
    print(f"{name} - Best Parameters: {grid_search.best_params_}")
    print(f"{name} - Best Score: {grid_search.best_score_}")
    print(f"{name} - Test Accuracy: {accuracy_score(y_test, y_pred)}")
    print(f"{name} - Confusion Matrix:\n{confusion_matrix(y_test, y_pred)}")
    print(f"{name} - Classification Report:\n{classification_report(y_test, y_pred)}")
    print('\n')

    if accuracy_score(y_test, y_pred) > best_accuracy:
        best_accuracy = accuracy_score(y_test, y_pred)
        best_model = grid_search.best_estimator_

# print the best model & accuracy
print(f"The Best model is {best_model.named_steps['model']} with an accuracy of {best_accuracy*100}%")


# Save the best model (optional)
# import pickle
# pickle.dump(best_model, open('best_model.pkl', 'wb'))

"""**Summary**

1.The minimum age to have a heart disease starts from 28 years old.

2.Most of the people get heart disease at the age of 53-54 years.

3.Most of the males and females get heart disease at the age of 54-55 years.

4.Male percentage in the data: 78.91%

5.Female Percentage in the data: 21.09%

6.Males are 274.23% more than females in the data.

7.We have highest number of people from Cleveland (304) and lowest from Switzerland (123).\ The highest number of females in this dataset are from Cleveland (97) and lowest from VA Long Beach (6).\ The highest number of males in this dataset are from Hungary (212) and lowest from Switzerland (113).

8.Majority of the Patients have Resting Blood pressure ranges from 110-150 mm Hg.

9.The majority of the patients have cholesterol levels between 200-300 mg/dl. Which is slightly higher than the normal range.

10.The majority of the patients have fasting blood sugar levels less than 120 mg/dl.

11.he majority of the patients have normal Resting ECG but some patients have ST-T wave abnormality. which may indicate heart issues.

12.The young age group has a higher heart rate as compared to the older age group.

13.The majority of the patients does not experience angina during physical exertion but age group 51-60 has the highest number of patients who experience angina during physical exertion.

14.ST depression (oldpeak) rises with age, showing a higher risk of heart issues in older age groups.\ Age groups 51-80 have average oldpeak values over 1 mm, indicating clinically significant heart stress.\ The 61-70 group has the highest average oldpeak (1.52 mm), suggesting increased heart disease risk in this age bracket.\ Males have higher oldpeak values as compared to Femlaes.

15.Younger Age Groups (0-40): Primarily exhibit upsloping and flat slopes, suggesting relatively healthier heart responses.\ Middle Age Groups (41-60): Higher counts of flat slopes (up to 177) indicate an increase in potential ischemia risk.\ Older Age Groups (61-80): A mix of slopes, with a notable presence of downsloping (8 cases in 71-80), indicating a concerning trend toward severe heart conditions.

16.Males show more affected vessels across all age bins.\ Significant rise in affected vessels with age, especially in males aged 51-60 (233 cases).\ Few cases in the 0-30 age group (1 female, 4 male).\ Notable increase in the 41-50 age group (144 males).

17.Very few cases (1 female, 4 males) in 0-30 age group, indicating low heart disease risk.\ Significant increase in 41-50 age group, especially in males (144 cases), indicating higher risk.\ Highest count in 51-60 age group (233 males), suggesting urgent monitoring.\ Females consistently show lower counts across all age bins.\ Notable cases in 61-70 age group (32 females, 106 males), highlighting increased risk.

18.Minimal heart disease predictions in the 0-30 age group.\ Significant cases (especially num values 1 and 2) in the 41-50 age group.\ Severe predictions rise in 51-60 and 61-70 age groups for males.\ Females show fewer predicted heart disease cases overall.

19.The model achieved an accuracy of over 90%, indicating strong predictive performance.\ The confusion matrix showed high true positives and true negatives, effectively distinguishing between patients with and without heart disease.\ Precision and recall were both high, minimizing false positives and ensuring most cases of heart disease were correctly identified.\ The F1 score was robust, balancing precision and recall.\ Feature importance analysis highlighted key factors impacting predictions, aiding in targeted healthcare strategies.

20.Imputing Missing Values:\ I imputed missing values using the Random Forest algorithm by training the model on features with complete data. The model predicted missing values based on the relationships learned from other features, providing more accurate imputation than simple methods.

21.Dealing with Outliers:\ I handled outliers using the Z-score method. I calculated the Z-scores for numeric features and identified outliers as those with Z-scores greater than 3 or less than -3. I then removed these outliers to ensure the model's accuracy and robustness.
"""